<!DOCTYPE html>
<html lang="zh-cn">
  <head>
    
<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />



  <meta name="description" content="Scrapy入门"/>




  <meta name="keywords" content="Scrapy," />




  <link rel="alternate" href="/atom.xml" title="Fynn's Blog" type="application/atom+xml">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=1.1.2" />



<link rel="canonical" href="https://fynn90.github.io/2017/11/19/scrapy入门/"/>


<meta name="description" content="Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序。其最初是为了页面抓取所设计的，也可以应用在获取API所返回的数据或者通用的网络爬虫。">
<meta property="og:type" content="article">
<meta property="og:title" content="Scrapy入门">
<meta property="og:url" content="https://fynn90.github.io/2017/11/19/scrapy%E5%85%A5%E9%97%A8/index.html">
<meta property="og:site_name" content="Fynn&#39;s Blog">
<meta property="og:description" content="Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序。其最初是为了页面抓取所设计的，也可以应用在获取API所返回的数据或者通用的网络爬虫。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic1.zhimg.com/v2-8c591d54457bb033812a2b0364011e9c_r.png">
<meta property="article:published_time" content="2017-11-19T14:56:00.000Z">
<meta property="article:modified_time" content="2019-10-31T01:48:27.267Z">
<meta property="article:author" content="Fynn">
<meta property="article:tag" content="Scrapy">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic1.zhimg.com/v2-8c591d54457bb033812a2b0364011e9c_r.png">


<link rel="stylesheet" type="text/css" href="/css/style.css?v=1.1.2" />



  <link rel="stylesheet" type="text/css" href="/lib/fancybox/jquery.fancybox.css" />





<script>
  var CONFIG = {
    search: false,
    searchPath: "/search.xml",
    fancybox: true,
    toc: true,
  }
</script>




  
  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?607980a031d3edcefed502ce80e77ffb";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>


  <script type="text/javascript">
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-115728733-1', 'auto');
        ga('send', 'pageview');
  </script>



    <title> Scrapy入门 · Fynn's Blog </title>
  <meta name="generator" content="Hexo 5.4.0"></head>

  <body><div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/." class="logo">Fynn's Blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>

<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    
      <a href="/">
        <li class="mobile-menu-item">
          
          
            Home
          
        </li>
      </a>
    
      <a href="/archives/">
        <li class="mobile-menu-item">
          
          
            Archives
          
        </li>
      </a>
    
      <a href="/tags">
        <li class="mobile-menu-item">
          
          
            Tags
          
        </li>
      </a>
    
  </ul>
</nav>

    <div class="container" id="mobile-panel">
      <header id="header" class="header"><div class="logo-wrapper">
  <a href="/." class="logo">Fynn's Blog</a>
</div>

<nav class="site-navbar">
  
    <ul id="menu" class="menu">
      
        <li class="menu-item">
          <a class="menu-item-link" href="/">
            
            
              首页
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/archives/">
            
            
              归档
            
          </a>
        </li>
      
        <li class="menu-item">
          <a class="menu-item-link" href="/tags">
            
            
              标签
            
          </a>
        </li>
      
      
    </ul>
  
</nav>

      </header>

      <main id="main" class="main">
        <div class="content-wrapper">
          <div id="content" class="content">
            
  
  <article class="post">
    <header class="post-header">
      <h1 class="post-title">
        
          Scrapy入门
        
      </h1>

      <div class="post-meta">
        <span class="post-time">
          2017年11月19日
        </span>
      </div>
    </header>

    
    
  <div class="post-toc" id="post-toc">
    <h2 class="post-toc-title">文章目录</h2>
    <div class="post-toc-content">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84%E6%A6%82%E8%A7%88"><span class="toc-text">架构概览</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Scrapy-Engine"><span class="toc-text">Scrapy Engine</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Scheduler-%E8%B0%83%E5%BA%A6%E5%99%A8"><span class="toc-text">Scheduler 调度器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Downloader-%E4%B8%8B%E8%BD%BD%E5%99%A8"><span class="toc-text">Downloader 下载器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spiders"><span class="toc-text">Spiders</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Item-Pipeline"><span class="toc-text">Item Pipeline</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Downloader-middlewares-%E4%B8%8B%E8%BD%BD%E5%99%A8%E4%B8%AD%E9%97%B4%E4%BB%B6"><span class="toc-text">Downloader middlewares 下载器中间件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spider%E4%B8%AD%E9%97%B4%E4%BB%B6%EF%BC%88Spider-middlewares%EF%BC%89"><span class="toc-text">Spider中间件（Spider middlewares）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Data-flow-%E6%95%B0%E6%8D%AE%E6%B5%81"><span class="toc-text">Data flow 数据流</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%89%E8%A3%85%E6%8C%87%E5%8D%97"><span class="toc-text">安装指南</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy%E5%85%A5%E9%97%A8"><span class="toc-text">Scrapy入门</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spider"><span class="toc-text">Spider</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%90%E8%A1%8Cspider"><span class="toc-text">运行spider</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#scrapy-Spider-scrapy-spiders-Spider"><span class="toc-text">scrapy.Spider (scrapy.spiders.Spider)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spider-%E5%8F%82%E6%95%B0"><span class="toc-text">Spider 参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E7%94%A8Spiders"><span class="toc-text">通用Spiders</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CrawSpider-%EF%BC%88class-scrapy-spiders-CrawlSpider%EF%BC%89"><span class="toc-text">CrawSpider （class scrapy.spiders.CrawlSpider）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#XMLFeedSpider"><span class="toc-text">XMLFeedSpider</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Selectors"><span class="toc-text">Selectors</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E9%80%89%E6%8B%A9%E5%99%A8"><span class="toc-text">使用选择器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BE%8B%E5%AD%90"><span class="toc-text">例子</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Item"><span class="toc-text">Item</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Item-Fields"><span class="toc-text">Item Fields</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E4%B8%80%E4%B8%AAItem"><span class="toc-text">创建一个Item</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E5%AD%97%E6%AE%B5%E5%80%BC"><span class="toc-text">获取字段值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%8E%B7%E5%8F%96%E6%89%80%E6%9C%89%E7%9A%84%E5%80%BC"><span class="toc-text">获取所有的值</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E5%AE%83%E4%BB%BB%E5%8A%A1"><span class="toc-text">其它任务</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Item-Loaders"><span class="toc-text">Item Loaders</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8Item-Loaders"><span class="toc-text">使用Item Loaders</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E5%92%8C%E8%BE%93%E5%87%BA%E5%A4%84%E7%90%86%E5%99%A8"><span class="toc-text">输入和输出处理器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A3%B0%E6%98%8EItem-Loader"><span class="toc-text">声明Item Loader</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A3%B0%E6%98%8E%E8%BE%93%E5%85%A5%E5%92%8C%E8%BE%93%E5%87%BA%E5%A4%84%E7%90%86%E5%99%A8"><span class="toc-text">声明输入和输出处理器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ItemLoader-Objects"><span class="toc-text">ItemLoader Objects</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Item-Pipeline-1"><span class="toc-text">Item Pipeline</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%99%E4%B8%AAitem-pipeline"><span class="toc-text">写个item pipeline</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%BB%E6%8E%89%E6%B2%A1%E6%9C%89%E4%BB%B7%E6%A0%BC%E7%9A%84item"><span class="toc-text">去掉没有价格的item</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B0%86items%E5%86%99%E5%85%A5JSON%E6%96%87%E4%BB%B6"><span class="toc-text">将items写入JSON文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%99%E5%85%A5-MongoDB"><span class="toc-text">写入 MongoDB</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8B%BF%E5%88%B0item%E7%9A%84%E5%BF%AB%E7%85%A7"><span class="toc-text">拿到item的快照</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8E%BB%E9%87%8D"><span class="toc-text">去重</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%BF%80%E6%B4%BBItem-Pipeline-%E7%BB%84%E4%BB%B6"><span class="toc-text">激活Item Pipeline 组件</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Settings"><span class="toc-text">Settings</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8C%87%E5%AE%9A%E8%AE%BE%E5%AE%9A"><span class="toc-text">指定设定</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A1%AB%E5%86%99settings"><span class="toc-text">填写settings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%91%BD%E4%BB%A4%E8%A1%8C%E9%80%89%E9%A1%B9-Command-line-options"><span class="toc-text">命令行选项(Command line options)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#spider%E5%86%85%E6%8C%87%E5%AE%9ASettings"><span class="toc-text">spider内指定Settings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AEsettings"><span class="toc-text">项目settings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%91%BD%E4%BB%A4%E9%BB%98%E8%AE%A4settings"><span class="toc-text">命令默认settings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%BB%98%E8%AE%A4%E5%85%A8%E5%B1%80settings"><span class="toc-text">默认全局settings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BF%E9%97%AEsettings"><span class="toc-text">访问settings</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%B7%A5%E5%85%B7"><span class="toc-text">命令行工具</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E6%95%B4%E8%AE%BE%E7%BD%AE"><span class="toc-text">调整设置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%BB%98%E8%AE%A4Scrapy%E9%A1%B9%E7%9B%AE%E7%BB%93%E6%9E%84"><span class="toc-text">默认Scrapy项目结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E9%A1%B9%E7%9B%AE"><span class="toc-text">创建项目</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BAspider"><span class="toc-text">创建spider</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E7%94%A8-spider"><span class="toc-text">调用 spider</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%97%E5%87%BA-spider"><span class="toc-text">列出 spider</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87%E9%93%BE%E6%8E%A5%E5%8A%A0%E8%BD%BD%E9%A1%B5%E9%9D%A2"><span class="toc-text">通过链接加载页面</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#spider%E6%96%B9%E5%BC%8F%E5%B1%95%E7%A4%BA%E9%A1%B5%E9%9D%A2"><span class="toc-text">spider方式展示页面</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A7%A3%E6%9E%90%E9%93%BE%E6%8E%A5"><span class="toc-text">解析链接</span></a></li></ol></li></ol>
    </div>
  </div>


    <div class="post-content">
      
        <p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序。<br>其最初是为了页面抓取所设计的，也可以应用在获取API所返回的数据或者通用的网络爬虫。  </p>
<span id="more"></span> 
<p><img src="https://pic1.zhimg.com/v2-8c591d54457bb033812a2b0364011e9c_r.png"></p>
<h2 id="架构概览"><a href="#架构概览" class="headerlink" title="架构概览"></a>架构概览</h2><h3 id="Scrapy-Engine"><a href="#Scrapy-Engine" class="headerlink" title="Scrapy Engine"></a>Scrapy Engine</h3><blockquote>
<p>引擎负责控制数据流在系统中所有组件中流动，并在相应动作发生时触发事件。详细内容查看下面的数据流部分。</p>
</blockquote>
<p>此组件相当于爬虫的”大脑”，是整个爬虫的调度中心。</p>
<h3 id="Scheduler-调度器"><a href="#Scheduler-调度器" class="headerlink" title="Scheduler 调度器"></a>Scheduler 调度器</h3><blockquote>
<p>调度器从引擎接收request并将他们入队，以便之后引擎请求他们时提供给引擎。</p>
</blockquote>
<p>初始的爬取URL和后续在页面中获取的待爬取的URL将放入调度器中，等待爬取。同时调度器会自动去除重复的URL（如果特定的URL不需要去重也可以通过设置实现，如post请求的URL）</p>
<h3 id="Downloader-下载器"><a href="#Downloader-下载器" class="headerlink" title="Downloader 下载器"></a>Downloader 下载器</h3><blockquote>
<p>下载器负责获取页面数据并提供给引擎，而后提供给spider。</p>
</blockquote>
<h3 id="Spiders"><a href="#Spiders" class="headerlink" title="Spiders"></a>Spiders</h3><blockquote>
<p>Spider是Scrapy用户编写用于分析response并提取item或额外跟进的URL的类。每个spider负责处理一个特定网站。</p>
</blockquote>
<h3 id="Item-Pipeline"><a href="#Item-Pipeline" class="headerlink" title="Item Pipeline"></a>Item Pipeline</h3><blockquote>
<p>Item Popeline负责处理被spider提取出来的item。典型的处理有清理、验证及持久化。</p>
</blockquote>
<p>当页面被爬虫解析所需的数据存入Item后，将被放送到项目管道（Pipeline）,经过几个特定的次序处理数据，最后存入本地文件或存入数据库。</p>
<h3 id="Downloader-middlewares-下载器中间件"><a href="#Downloader-middlewares-下载器中间件" class="headerlink" title="Downloader middlewares 下载器中间件"></a>Downloader middlewares 下载器中间件</h3><blockquote>
<p>下载器中间件是在引擎及下载器之间的特定钩子，处理Downloader传递给引擎的response。其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。  </p>
</blockquote>
<p>通过设置下载器中间件可以实现爬虫自动更换user-agent、IP等功能。</p>
<h3 id="Spider中间件（Spider-middlewares）"><a href="#Spider中间件（Spider-middlewares）" class="headerlink" title="Spider中间件（Spider middlewares）"></a>Spider中间件（Spider middlewares）</h3><blockquote>
<p>Spider中间件是在引擎及Spider之间的特定钩子，处理spider的输入和输出。其提供了一个简便的机制，通过插入自定义代码来扩展Scrapy功能。</p>
</blockquote>
<h3 id="Data-flow-数据流"><a href="#Data-flow-数据流" class="headerlink" title="Data flow 数据流"></a>Data flow 数据流</h3><ol>
<li> 引擎打开一个网站，找到处理该网站的Spider并向该spider请求第一个要爬取的URL。</li>
</ol>
<ul>
<li>引擎从Spider中获取到第一个要爬取的URL并在调度器（Scheduler）以Request调度。</li>
<li>引擎向调度器请求下一个要爬取的URL</li>
<li>调度器返回下一个要爬取的URL给引擎，引擎将URL通过下载中间件转发给下载器。</li>
<li>一旦页面下载完毕，下载器生成一个该页面的Response,并将其通过下载中间件（返回response方向）发送给引擎。</li>
<li>引擎从下载器中接收到Response并通过Spider中间件发送给Spider处理。</li>
<li>Spider处理Response并返回爬取到的Item及新的Request给引擎。</li>
<li>引擎将（Spider返回的）爬取到的Item给Item Pipline,将（Spider返回的）Request给调度器。</li>
<li>（从第二步）重复直到调度器中没有更多地request,引擎关闭该网站。</li>
</ul>
<h2 id="安装指南"><a href="#安装指南" class="headerlink" title="安装指南"></a>安装指南</h2><p>Scrapy最新版本支持Python2.7,Python3.3+。通过pip安装：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install Scrapy <span class="comment"># python -m pip install Scrapy</span></span><br></pre></td></tr></table></figure>    
<h2 id="Scrapy入门"><a href="#Scrapy入门" class="headerlink" title="Scrapy入门"></a>Scrapy入门</h2><p>新建一个Scrapy项目。  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject tutorial</span><br></pre></td></tr></table></figure>    
<p>该命令将创建包含下列内容__tutorial__目录：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">tutorial/</span><br><span class="line">	scrapy.cfg #项目的配置文件</span><br><span class="line">	tutorial/ # 该项目的python模块</span><br><span class="line">		__init__.py</span><br><span class="line">		items.py # item</span><br><span class="line">		pipelines.py # pipeline</span><br><span class="line">		settings.py # 项目的设置文件</span><br><span class="line">		spiders/ # spider目录</span><br><span class="line">			__init__.py</span><br><span class="line">			...</span><br></pre></td></tr></table></figure>                
<h2 id="Spider"><a href="#Spider" class="headerlink" title="Spider"></a><a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/spiders.html">Spider</a></h2><p>Spiders是你定义的类，它们被用于爬取网站的信息。它们必须继承于<code>scrapy.Spider</code>类。<br>Spiders类定义了如何跟进页面的链接以及如何分析页面中的内容，提取生成<code>item</code>的方法。<br>编写第一个Spider,文件名字是<code>quotes_spider.py</code>，放入<code>tutorial/spiders</code>目录下：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QuotesSpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">	name = <span class="string">&#x27;quotes&#x27;</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">start_requests</span>(<span class="params">self</span>):</span></span><br><span class="line">		urls = [</span><br><span class="line">				<span class="string">&#x27;http://quotes.toscrape.com/page/1/&#x27;</span>,</span><br><span class="line">				<span class="string">&#x27;http://quotes.toscrape.com/page/2/&#x27;</span></span><br><span class="line">			]</span><br><span class="line">		<span class="keyword">for</span> url <span class="keyword">in</span> urls:</span><br><span class="line">			<span class="keyword">yield</span> scrapy.Request(url=url, callback=self.parse)</span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">		page = response.url.split(<span class="string">&quot;/&quot;</span>)[-<span class="number">2</span>]</span><br><span class="line">		filename = <span class="string">&#x27;quotes-%s.html&#x27;</span> % page</span><br><span class="line">		<span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">		f.write(response.body)</span><br><span class="line">		self.log(<span class="string">&#x27;Saved file %s&#x27;</span> % filename)</span><br></pre></td></tr></table></figure>            
<p><code>scrapy.Spider</code>子类，需要定义一些属性和方法：</p>
<ul>
<li>__name：__Spider名称。用于在Spiders中跟踪Spider，它应该是唯一性的。</li>
<li><strong>start_requests()：</strong>    必须返回一个可迭代的请求对象（你也可以返回一个列表类型的请求对象或一个生成器方法），Spider会调用Request对象开始爬取。      </li>
<li>__parse()：__当请求完成时，会调用此方法并将返回内容(response)传入。response是__TextResponse__实例，里面包含页面内容和一些方法。</li>
</ul>
<p><code>parse()</code>方法一般用于解析response对象，提取爬取的数据存入字典，它也可以找到新URL加入爬取队列中。</p>
<h3 id="运行spider"><a href="#运行spider" class="headerlink" title="运行spider"></a>运行spider</h3><p>在项目跟目录指向：  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl quotes</span><br></pre></td></tr></table></figure>    
<h3 id="scrapy-Spider-scrapy-spiders-Spider"><a href="#scrapy-Spider-scrapy-spiders-Spider" class="headerlink" title="scrapy.Spider (scrapy.spiders.Spider)"></a>scrapy.Spider (scrapy.spiders.Spider)</h3><p>这是最简单的spider，任何个spider都需要继承它。它并不会提供特殊的方法，它只默认提供<code>start_requests()</code>实现。<code>start_requests()</code>会从<code>start_urls</code>读取请求链接发起请求，当请求完成后会自动调用<code>parse</code>方法将返回结果作为参数<code>response</code>传入。  </p>
<blockquote>
<p><strong>name</strong></p>
</blockquote>
<p>定义spider名字，它被用于Scrapy定位spider,所以它必须是唯一的。<br>如果spider被用于爬取单个网站，一个最常见的做法是以该网站域名来命名spider。例如：<code>mywebsite.com</code>，该<code>spider</code>通常被命名未<code>mywebsite</code>。</p>
<blockquote>
<p><strong>allowed_domains</strong></p>
</blockquote>
<p>一组可以爬取的域名字符串，如果<a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/spider-middleware.html#scrapy.spidermiddlewares.offsite.OffsiteMiddleware"><code>OffsiteMiddleware</code></a>开启，不在此列表中的域名不会被爬取。</p>
<blockquote>
<p><strong>start_urls</strong><br>URL列表，如果没有特殊URL被定义，spider会先从这个列表取URL进行爬取，后续的URL会从爬取的页面中获取到。</p>
</blockquote>
<blockquote>
<p>__custom_settings</p>
</blockquote>
<p>定义当前spider配置，它会覆盖项目配置。它必须被定义未类属性，因为它会在实例化前使用到。</p>
<blockquote>
<p><strong>start_requests()</strong></p>
</blockquote>
<p>整个方法必须返回一个可迭代对象。该对象包含spider用于爬取的第一个Request。<br>当spider启动爬取并且未制定URL时，该方法被调用。<br>默认会对<code>start\_urls</code>url一次执行<code>Request(url,dont\_filter=true)</code><br>如果您想要修改最初爬取某个网站的Request对象，您可以重写(override)该方法。 例如，如果您需要在启动时以POST登录某个网站，你可以这么写:  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">	name = <span class="string">&#x27;myspider&#x27;</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">start_requests</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="keyword">return</span> [scrapy.FormRequest(<span class="string">&quot;http://www.example.com/login&quot;</span>,</span><br><span class="line">									formdata=&#123;<span class="string">&#x27;user&#x27;</span>: <span class="string">&#x27;john&#x27;</span>, <span class="string">&#x27;pass&#x27;</span>: <span class="string">&#x27;secret&#x27;</span>&#125;,</span><br><span class="line">									callback=self.logged_in)]</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">logged_in</span>(<span class="params">self, response</span>):</span></span><br><span class="line">		<span class="comment"># here you would extract links to follow and return Requests for</span></span><br><span class="line">		<span class="comment"># each of them, with another callback</span></span><br><span class="line">		<span class="keyword">pass</span></span><br></pre></td></tr></table></figure>            
<h3 id="Spider-参数"><a href="#Spider-参数" class="headerlink" title="Spider 参数"></a>Spider 参数</h3><p>Spiders能够接收参数并编辑它们。有些用户会同过参数定义spider起始URL或者限制爬虫爬取页面的区域。<br>它们也可以被用于配置spider任何功能。  </p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl myspider -a category=electronics</span><br></pre></td></tr></table></figure>    
<p>Spider中<code>__init__</code>方法接收传入的参数：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">	name = <span class="string">&#x27;myspider&#x27;</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, category=<span class="literal">None</span>, *args, **kwargs</span>):</span></span><br><span class="line">		<span class="built_in">super</span>(MySpider, self).__init__(*args, **kwargs)</span><br><span class="line">		self.start_urls = [<span class="string">&#x27;http://www.example.com/categories/%s&#x27;</span> % category]</span><br></pre></td></tr></table></figure>            
<p><code>__init__</code>    方法默认的会将接收到的参数赋值个spider一个同名属性。所以上面的例子可以改写为：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">	name = <span class="string">&#x27;myspider&#x27;</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">start_requests</span>(<span class="params">self</span>):</span></span><br><span class="line">		<span class="keyword">yield</span> scrapy.Request(<span class="string">&#x27;http://www.example.com/categories/%s&#x27;</span> % self.category)</span><br></pre></td></tr></table></figure>            
<h3 id="通用Spiders"><a href="#通用Spiders" class="headerlink" title="通用Spiders"></a>通用Spiders</h3><p>Scrapy附带一些有用的通用蜘蛛，你可以继承它们。它们为一些常见的案例提供便捷的功能。<br>例如遵循特定的规则爬取站点链接，解析XML/CSV。</p>
<h3 id="CrawSpider-（class-scrapy-spiders-CrawlSpider）"><a href="#CrawSpider-（class-scrapy-spiders-CrawlSpider）" class="headerlink" title="CrawSpider （class scrapy.spiders.CrawlSpider）"></a>CrawSpider （class scrapy.spiders.CrawlSpider）</h3><p>爬取一般网站用的spider。定义一些规则来提供跟进link的方便机制。也许该spider不完全适用于你的站点，可以根据自己的需求修改部分方法。当然你也可以实现自己的spider。  </p>
<blockquote>
<p><strong>rules</strong></p>
</blockquote>
<p>rules是个包含一个或多个Rule对象的列表。每个Rule为爬虫定义爬取网站的行为规则。如果有多个rule匹配到同一个链接，第一个匹配到的规则会使用它。</p>
<p>这个spider暴露了一个可以覆盖的方法：  </p>
<blockquote>
<p><strong>parse_start_url(response)</strong></p>
</blockquote>
<p>当__start_urls__链接有返回时，这个方法会被调用。这个方法能够解析返回值并且必须返回一个<code>item</code>对象或一个<code>Request</code>对象或一个可迭代的包含两者的对象。<br><strong>爬取规则Crawling rules</strong><br><code>class scrapy.spiders.Rule(link_extractor, callback=None, cb_kwargs=None, follow=None, process_links=None, process_request=None)</code>  </p>
<ul>
<li><code>link_extractor</code> 一个 <a target="_blank" rel="noopener" href="https://docs.scrapy.org/en/latest/topics/link-extractors.html#topics-link-extractors">Link Extractor</a>对象，它定义了如何从页面从提取链接。</li>
<li><code>callback</code> 是个可执行方法或一个字符串（在spider定义了的方法）。它会在提取到符合要求的链接时触发。这个方法接收一个response作为第一个参数，它必须返回一个包含<code>item</code>或<code>Request</code>对象的列表。（注意：这个方法名称不能是__parse__）</li>
<li><code>cb_kwargs</code>包含传递给回调函数的参数(keyword argument)的字典。</li>
<li><code>follow</code> 一个布尔值。说明从返回中提取的符合规则的链接是否跟进。如果<code>callback</code>值是None则它的值默认是<code>True</code>,否则是<code>False</code></li>
<li><code>process_links</code>是个可执行方法或一个字符串（在spider定义了的方法）。每当<code>link\_extractor</code>的一个链接获取到reponse它就会被调用。它一般用于过滤。</li>
<li><code>process_request</code>是个可执行方法或一个字符串（在spider定义了的方法）。每当<code>link\_extractor</code>一个链接发起request时它就会被调用。它必须返回一个request对象或None  </li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> CrawlSpider, Rule</span><br><span class="line"><span class="keyword">from</span> scrapy.linkextractors <span class="keyword">import</span> LinkExtractor</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span>(<span class="params">CrawlSpider</span>):</span></span><br><span class="line">name = <span class="string">&#x27;example.com&#x27;</span></span><br><span class="line">allowed_domains = [<span class="string">&#x27;example.com&#x27;</span>]</span><br><span class="line">start_urls = [<span class="string">&#x27;http://www.example.com&#x27;</span>]</span><br><span class="line"></span><br><span class="line">rules = (</span><br><span class="line">	<span class="comment"># Extract links matching &#x27;category.php&#x27; (but not matching &#x27;subsection.php&#x27;)</span></span><br><span class="line">	<span class="comment"># and follow links from them (since no callback means follow=True by default).</span></span><br><span class="line">	Rule(LinkExtractor(allow=(<span class="string">&#x27;category\.php&#x27;</span>, ), deny=(<span class="string">&#x27;subsection\.php&#x27;</span>, ))),</span><br><span class="line"></span><br><span class="line">	<span class="comment"># Extract links matching &#x27;item.php&#x27; and parse them with the spider&#x27;s method parse_item</span></span><br><span class="line">	Rule(LinkExtractor(allow=(<span class="string">&#x27;item\.php&#x27;</span>, )), callback=<span class="string">&#x27;parse_item&#x27;</span>),</span><br><span class="line">)</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_item</span>(<span class="params">self, response</span>):</span></span><br><span class="line">	self.logger.info(<span class="string">&#x27;Hi, this is an item page! %s&#x27;</span>, response.url)</span><br><span class="line">	item = scrapy.Item()</span><br><span class="line">	item[<span class="string">&#x27;id&#x27;</span>] = response.xpath(<span class="string">&#x27;//td[@id=&quot;item_id&quot;]/text()&#x27;</span>).re(<span class="string">r&#x27;ID: (\d+)&#x27;</span>)</span><br><span class="line">	item[<span class="string">&#x27;name&#x27;</span>] = response.xpath(<span class="string">&#x27;//td[@id=&quot;item_name&quot;]/text()&#x27;</span>).extract()</span><br><span class="line">	item[<span class="string">&#x27;description&#x27;</span>] = response.xpath(<span class="string">&#x27;//td[@id=&quot;item_description&quot;]/text()&#x27;</span>).extract()</span><br><span class="line">	<span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<h3 id="XMLFeedSpider"><a href="#XMLFeedSpider" class="headerlink" title="XMLFeedSpider"></a><a target="_blank" rel="noopener" href="https://doc.scrapy.org/en/latest/topics/spiders.html">XMLFeedSpider</a></h3><p>XMLFeedSpider被设计为通过迭代确定的节点来解析XML源。迭代器能够选择为__iternodes__,<strong>xml</strong>,<strong>html__。<br>从性能上考虑推荐使用__iternodes__，因为__xml</strong>,__html__迭代器读取整个DOM用来解析。但是，不过使用__html__作为迭代器能有效应对错误的XML。  </p>
<p>你必须设置一下属性确定使用的迭代器和标签名。</p>
<blockquote>
<p><strong>iterator</strong><br>使用的迭代器名字，有三个选项：  </p>
</blockquote>
<ul>
<li><strong>iternodes：</strong> 基于正则表达很快的一个迭代器。</li>
<li><strong>html：</strong> 基于__Selector__的正则表达式。需要注意的是，它需要将DOM全部加载到内存解析，在大数据源情况下可能会有问题。</li>
<li><strong>xml：</strong> 基于__Selector__的正则表达式。需要注意的是，它需要将DOM全部加载到内存解析，在大数据源情况下可能会有问题。</li>
</ul>
<blockquote>
<p><strong>itertag</strong></p>
</blockquote>
<p>定义需要迭代的节点名称。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">itertag = <span class="string">&#x27;product&#x27;</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>namespaces</strong> ?</p>
</blockquote>
<p>有元组__(prefix, url)__组成的列表，它了定义这个文档可用的名称空间，这些元组将被spider处理。__prefix__和__uri__会自动被注册名称空间通过使用__register_namespace()__方法。<br>在__itertag__属性上指定节点的名称空间。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">YourSpider</span>(<span class="params">XMLFeedSpider</span>):</span></span><br><span class="line">	namespaces = [(<span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;http://www.sitemaps.org/schemas/sitemap/0.9&#x27;</span>)]</span><br><span class="line">	itertag = <span class="string">&#x27;n:url&#x27;</span></span><br><span class="line">	<span class="comment"># ...</span></span><br></pre></td></tr></table></figure>        
<p>除了这些新的属性，spider还提供了下面这些可以覆盖的方法。</p>
<blockquote>
<p><strong>adapt_response(response)</strong></p>
</blockquote>
<p>这个方法在spider分析response前被调用。你可以在这个方法编辑response主体在spider解析它之前。这个方法接收response并放回response。</p>
<blockquote>
<p><strong>parse_node(response, selector)</strong></p>
</blockquote>
<p>当迭代的节点和__itertag__相匹配时这个方法会被触发。每个节点会接收到__response__和__Selector__作为参数。这个方法是强制被重写的，否则spider不会正常工作。这个方法必须返回一个__item__对象或者一个__Request__对象，或者一个包含这两者的可迭代对象。</p>
<blockquote>
<p><strong>process_results(response, results)</strong></p>
</blockquote>
<p>当spider返回一个结果（item或者request）时，这个方法会被调用。它的目的是返回的结果在被传给框架核心前做最后的请求处理。例如设置item ID。它接收一个结果列表和对应的响应。它必须返回一个结果（Item or Request）列表。</p>
<p>XMLFeedSpider 例子：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.spiders <span class="keyword">import</span> XMLFeedSpider</span><br><span class="line"><span class="keyword">from</span> myproject.items <span class="keyword">import</span> TestItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span>(<span class="params">XMLFeedSpider</span>):</span></span><br><span class="line">	name = <span class="string">&#x27;example.com&#x27;</span></span><br><span class="line">	allowed_domains = [<span class="string">&#x27;example.com&#x27;</span>]</span><br><span class="line">	start_urls = [<span class="string">&#x27;http://www.example.com/feed.xml&#x27;</span>]</span><br><span class="line">	iterator = <span class="string">&#x27;iternodes&#x27;</span>  <span class="comment"># This is actually unnecessary, since it&#x27;s the default value</span></span><br><span class="line">	itertag = <span class="string">&#x27;item&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_node</span>(<span class="params">self, response, node</span>):</span></span><br><span class="line">	self.logger.info(<span class="string">&#x27;Hi, this is a &lt;%s&gt; node!: %s&#x27;</span>, self.itertag, <span class="string">&#x27;&#x27;</span>.join(node.extract()))</span><br><span class="line">	item = TestItem()</span><br><span class="line">	item[<span class="string">&#x27;id&#x27;</span>] = node.xpath(<span class="string">&#x27;@id&#x27;</span>).extract()</span><br><span class="line">	item[<span class="string">&#x27;name&#x27;</span>] = node.xpath(<span class="string">&#x27;name&#x27;</span>).extract()</span><br><span class="line">	item[<span class="string">&#x27;description&#x27;</span>] = node.xpath(<span class="string">&#x27;description&#x27;</span>).extract()</span><br><span class="line">	<span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>            
<h2 id="Selectors"><a href="#Selectors" class="headerlink" title="Selectors"></a><a target="_blank" rel="noopener" href="https://doc.scrapy.org/en/latest/topics/selectors.html">Selectors</a></h2><p>在爬取页面时，做的最多的任务就是从页面中提取需要的数据。完成这个任务最常用的库有：__BeautifulSoup__，__lxml__。<br>Scrapy有自己的提取数据的机制。它被成为选择器(selectors)。通过指定的__Xpath__或__CSS__表达式选取文档中符合要求的部分。<br>Scrapy选择器基于lxml库，它非常简单和快速。</p>
<h3 id="使用选择器"><a href="#使用选择器" class="headerlink" title="使用选择器"></a>使用选择器</h3><p>Scrapy选择器时__Selector__类的实例，通过传递一个text或__TextResponse__对象完成构造。它会根据输入内容自动选择最佳的解析规则（XML vs HTML）。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.selector <span class="keyword">import</span> Selector</span><br><span class="line"><span class="keyword">from</span> scrapy.http <span class="keyword">import</span> HtmlResponse</span><br><span class="line">body = <span class="string">&#x27;&lt;html&gt;&lt;body&gt;&lt;span&gt;good&lt;/span&gt;&lt;/body&gt;&lt;/html&gt;&#x27;</span></span><br><span class="line">Selector(text=body).xpath(<span class="string">&#x27;//span/text()&#x27;</span>).extract() <span class="comment"># [u&#x27;good&#x27;] 使用Text完成构造</span></span><br><span class="line">response = HtmlResponse(url=<span class="string">&#x27;http://example.com&#x27;</span>, body=body) <span class="comment"># 通过response完成构造</span></span><br><span class="line">Selector(response=response).xpath(<span class="string">&#x27;//span/text()&#x27;</span>).extract()</span><br></pre></td></tr></table></figure>    
<p>为了方便，__response__对象提供了选择器 __.selector__属性。这是个完全可靠的便捷方式。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response.selector.xpath(<span class="string">&#x27;//span.text()&#x27;</span>).extract()</span><br></pre></td></tr></table></figure>    
<h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>通过scrapy shell获取response。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">scrapy shell http://doc.scrapy.org/en/latest/_static/selectors-sample1.html</span><br><span class="line"></span><br><span class="line"><span class="comment">#&lt;html&gt;</span></span><br><span class="line"><span class="comment">#	&lt;head&gt;</span></span><br><span class="line"><span class="comment">#		&lt;base href=&#x27;http://example.com/&#x27; /&gt;</span></span><br><span class="line"><span class="comment">#		&lt;title&gt;Example website&lt;/title&gt;</span></span><br><span class="line"><span class="comment">#	&lt;/head&gt;</span></span><br><span class="line"><span class="comment">#	&lt;body&gt;</span></span><br><span class="line"><span class="comment">#		&lt;div id=&#x27;images&#x27;&gt;</span></span><br><span class="line"><span class="comment">#			&lt;a href=&#x27;image1.html&#x27;&gt;Name: My image 1 &lt;br /&gt;&lt;img src=&#x27;image1_thumb.jpg&#x27; /&gt;&lt;/a&gt;</span></span><br><span class="line"><span class="comment">#			&lt;a href=&#x27;image2.html&#x27;&gt;Name: My image 2 &lt;br /&gt;&lt;img src=&#x27;image2_thumb.jpg&#x27; /&gt;&lt;/a&gt;</span></span><br><span class="line"><span class="comment">#			&lt;a href=&#x27;image3.html&#x27;&gt;Name: My image 3 &lt;br /&gt;&lt;img src=&#x27;image3_thumb.jpg&#x27; /&gt;&lt;/a&gt;</span></span><br><span class="line"><span class="comment">#			&lt;a href=&#x27;image4.html&#x27;&gt;Name: My image 4 &lt;br /&gt;&lt;img src=&#x27;image4_thumb.jpg&#x27; /&gt;&lt;/a&gt;</span></span><br><span class="line"><span class="comment">#			&lt;a href=&#x27;image5.html&#x27;&gt;Name: My image 5 &lt;br /&gt;&lt;img src=&#x27;image5_thumb.jpg&#x27; /&gt;&lt;/a&gt;</span></span><br><span class="line"><span class="comment">#		&lt;/div&gt;</span></span><br><span class="line"><span class="comment">#	&lt;/body&gt;</span></span><br><span class="line"><span class="comment">#&lt;/html&gt;</span></span><br></pre></td></tr></table></figure>            
<p>获取页面标题：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">response.selector.xpath(<span class="string">&#x27;//title/text()&#x27;</span>)</span><br><span class="line">response.selector.css(<span class="string">&#x27;title::text&#x27;</span>)</span><br><span class="line">response.xpath(<span class="string">&#x27;//title/text()&#x27;</span>) <span class="comment">#更便捷的写法</span></span><br><span class="line">response.css(<span class="string">&#x27;title::text&#x27;</span>) <span class="comment">#更便捷的写法</span></span><br></pre></td></tr></table></figure>    
<p><code>.xpath()</code>和<code>.css()</code>方法返回的是个<code>SelectorList</code>实例。<br>提取文本数据需要调用<code>.extract()</code>方法。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response.xpath(<span class="string">&#x27;//title/text()&#x27;</span>).extract() <span class="comment"># [u&#x27;Example website&#x27;]</span></span><br></pre></td></tr></table></figure>    
<p>如果你想获取匹配元素中第一个元素文本可以使用<code>.extract_first()</code>  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response.xpath(<span class="string">&#x27;//div[@id=&quot;images&quot;]/a/text()&#x27;</span>).extract_first()</span><br></pre></td></tr></table></figure>    
<p>如果没有匹配元素存在会放回<code>None</code>  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">response.xpath(<span class="string">&#x27;//div[@id=&quot;not-exists&quot;]/text()&#x27;</span>).extract_first() <span class="keyword">is</span> <span class="literal">None</span> <span class="comment">#True</span></span><br></pre></td></tr></table></figure>    
<p>可以给不存在的元素设置一个默认返回值。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">response.xpath(<span class="string">&#x27;//div[@id=&quot;not-exists&quot;]/text()&#x27;</span>).extract_first(default=<span class="string">&#x27;not-found&#x27;</span>)</span><br><span class="line">response.xpath(<span class="string">&#x27;//base/@href&#x27;</span>).extract()</span><br><span class="line">[<span class="string">u&#x27;http://example.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">response.css(<span class="string">&#x27;base::attr(href)&#x27;</span>).extract()</span><br><span class="line">[<span class="string">u&#x27;http://example.com/&#x27;</span>]</span><br><span class="line"></span><br><span class="line">response.xpath(<span class="string">&#x27;//a[contains(@href, &quot;image&quot;)]/@href&#x27;</span>).extract()</span><br><span class="line">[<span class="string">u&#x27;image1.html&#x27;</span>,</span><br><span class="line"><span class="string">u&#x27;image2.html&#x27;</span>,</span><br><span class="line"><span class="string">u&#x27;image3.html&#x27;</span>,</span><br><span class="line"><span class="string">u&#x27;image4.html&#x27;</span>,</span><br><span class="line"><span class="string">u&#x27;image5.html&#x27;</span>]</span><br><span class="line"></span><br><span class="line">response.css(<span class="string">&#x27;a[href*=image]::attr(href)&#x27;</span>).extract()</span><br><span class="line">[<span class="string">u&#x27;image1.html&#x27;</span>,</span><br><span class="line"><span class="string">u&#x27;image2.html&#x27;</span>,</span><br><span class="line"><span class="string">u&#x27;image3.html&#x27;</span>,</span><br><span class="line"><span class="string">u&#x27;image4.html&#x27;</span>,</span><br><span class="line"><span class="string">u&#x27;image5.html&#x27;</span>]</span><br><span class="line"></span><br><span class="line">response.xpath(<span class="string">&#x27;//a[contains(@href, &quot;image&quot;)]/img/@src&#x27;</span>).extract()</span><br><span class="line">[<span class="string">u&#x27;image1_thumb.jpg&#x27;</span>,</span><br><span class="line"><span class="string">u&#x27;image2_thumb.jpg&#x27;</span>,</span><br><span class="line"><span class="string">u&#x27;image3_thumb.jpg&#x27;</span>,</span><br><span class="line"><span class="string">u&#x27;image4_thumb.jpg&#x27;</span>,</span><br><span class="line"><span class="string">u&#x27;image5_thumb.jpg&#x27;</span>]</span><br><span class="line"></span><br><span class="line">response.css(<span class="string">&#x27;a[href*=image] img::attr(src)&#x27;</span>).extract()</span><br><span class="line">[<span class="string">u&#x27;image1_thumb.jpg&#x27;</span>,</span><br><span class="line"><span class="string">u&#x27;image2_thumb.jpg&#x27;</span>,</span><br><span class="line"><span class="string">u&#x27;image3_thumb.jpg&#x27;</span>,</span><br><span class="line"><span class="string">u&#x27;image4_thumb.jpg&#x27;</span>,</span><br><span class="line"><span class="string">u&#x27;image5_thumb.jpg&#x27;</span>]</span><br></pre></td></tr></table></figure>    
<p>选择器有个<code>re()</code>方法支持用正则表达式提取数据。<code>re()</code>方法返回的是个unicode字符串列表。</p>
<h2 id="Item"><a href="#Item" class="headerlink" title="Item"></a>Item</h2><p>Item是保存爬取到的数据的容器；其使用方法和python字典类似。Scrapy中可以直接使用dict，但是<code>Item</code>提供了额外保护机制来避免拼写错误导致的未定义字段错误。<code>Item</code>也可以于<code>Item Loaders</code>一起使用，<code>item Loaders</code>可以快速将数据填充进<code>Item</code>。<br>根据可能获取到的数据对<code>item</code>进行建模。    </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Product</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">   		name = scrapy.Field()</span><br><span class="line">   		price = scrapy.Field()</span><br><span class="line">   		stock = scrapy.Field()</span><br><span class="line">	last_updated = scrapy.Field(serializer=<span class="built_in">str</span>)</span><br></pre></td></tr></table></figure>        
<h3 id="Item-Fields"><a href="#Item-Fields" class="headerlink" title="Item Fields"></a>Item Fields</h3><p><code>Field</code>对象用于指定每个字段的元数据。例如，上面的例子中<code>last_updated</code>所示字段的序列化函数。<br>你可以给字段指定任何类型的元数据。<code>Field</code>对象接收的值没有限制，因为这个原因，没有可用的元数据key的参考列表。<br><code>Field</code>对象定义的key可以被用在不同的组件中，只要组件知道它们用途。</p>
<h3 id="创建一个Item"><a href="#创建一个Item" class="headerlink" title="创建一个Item"></a>创建一个Item</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">product = Product(name = <span class="string">&#x27;PC&#x27;</span>,price = <span class="number">1000</span>)</span><br><span class="line"><span class="built_in">print</span> product <span class="comment"># Product(name=&#x27;PC&#x27;,price=1000)</span></span><br></pre></td></tr></table></figure>    
<h3 id="获取字段值"><a href="#获取字段值" class="headerlink" title="获取字段值"></a>获取字段值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">product[<span class="string">&#x27;name&#x27;</span>] <span class="comment"># PC</span></span><br><span class="line">product.get(<span class="string">&#x27;name&#x27;</span>) <span class="comment"># PC</span></span><br><span class="line">product.get(<span class="string">&#x27;last_updated&#x27;</span>, <span class="string">&#x27;not set&#x27;</span>) <span class="comment"># 如果没有赋值的字段 设置一个默认值</span></span><br><span class="line"><span class="string">&#x27;name&#x27;</span> <span class="keyword">in</span> product <span class="comment">#True 判断字段是否被赋值</span></span><br><span class="line"><span class="string">&#x27;last_updated&#x27;</span> <span class="keyword">in</span> product <span class="comment">#False</span></span><br><span class="line"><span class="string">&#x27;last_updated&#x27;</span> <span class="keyword">in</span> product.fields <span class="comment">#True 判断此字段是否声明</span></span><br></pre></td></tr></table></figure>    
<h3 id="获取所有的值"><a href="#获取所有的值" class="headerlink" title="获取所有的值"></a>获取所有的值</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">product.keys()</span><br><span class="line">[<span class="string">&#x27;price&#x27;</span>,<span class="string">&#x27;name&#x27;</span>]</span><br><span class="line">product.items()</span><br><span class="line">[(<span class="string">&#x27;price&#x27;</span>,<span class="number">1000</span>),(<span class="string">&#x27;name&#x27;</span>,<span class="string">&#x27;PC&#x27;</span>)]</span><br></pre></td></tr></table></figure>    
<h3 id="其它任务"><a href="#其它任务" class="headerlink" title="其它任务"></a>其它任务</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">product2 = Product(product) <span class="comment"># 赋值item</span></span><br><span class="line"><span class="built_in">print</span> product2 <span class="comment">#Product(name=&#x27;PC&#x27;,price=1000) #赋值item</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">dict</span>(product) <span class="comment"># &#123;&#x27;price&#x27;:1000,&#x27;name&#x27;:&#x27;PC&#x27;&#125; # 创建字典</span></span><br></pre></td></tr></table></figure>    
<h2 id="Item-Loaders"><a href="#Item-Loaders" class="headerlink" title="Item Loaders"></a><a target="_blank" rel="noopener" href="https://doc.scrapy.org/en/latest/topics/loaders.html">Item Loaders</a></h2><p>Loader提供一个方便的机制用于填充scraped Items。尽管Items可以利用类似于dicAPI填充数据，但Loader提供更方便的API用于在爬取过程中填充。<br>换句话说，__Items__提供装爬取数据的容器而item Loader提供填充容器的机制。<br>Item Loader提供灵活，有效，简单的机制用于扩展和重写不用的字段解析规则，无论是spider还是源格式（HTML,X,ML）。避免了后期噩梦般的维护。</p>
<h3 id="使用Item-Loaders"><a href="#使用Item-Loaders" class="headerlink" title="使用Item Loaders"></a>使用Item Loaders</h3><p>要使用一个Item Loader,你必须首先实例化它。你可以像字典对象一样实例化也可以没有对象，这样情况下Item Loader构造时自动使用一个Item实例化，这个Item被赋予<code>ItemLoader.default_item_class</code>属性。<br>然后，你就可以向Item Loader里面收集数据了，通常的做法是使用<code>Selectors</code>。你给一个字段添加多个值。Item Loader会使用合适的处理函数加载这些值。<br>下面例子，将Item Loader用于Spider,使用了 Product item定义的 Items。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"><span class="keyword">from</span> myproject.items <span class="keyword">import</span> Product</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">	l = ItemLoader(item=Product(), response=response)</span><br><span class="line">	l.add_xpath(<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;//div[@class=&quot;product_name&quot;]&#x27;</span>)</span><br><span class="line">	l.add_xpath(<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;//div[@class=&quot;product_title&quot;]&#x27;</span>)</span><br><span class="line">	l.add_xpath(<span class="string">&#x27;price&#x27;</span>, <span class="string">&#x27;//p[@id=&quot;price&quot;]&#x27;</span>)</span><br><span class="line">	l.add_css(<span class="string">&#x27;stock&#x27;</span>, <span class="string">&#x27;p#stock]&#x27;</span>)</span><br><span class="line">	l.add_value(<span class="string">&#x27;last_updated&#x27;</span>, <span class="string">&#x27;today&#x27;</span>) <span class="comment"># you can also use literal values</span></span><br><span class="line">	<span class="keyword">return</span> l.load_item()</span><br></pre></td></tr></table></figure>        
<p>上面例子中，通过<code>add_xpath()</code>方法，在页面两个不同的地方将数据收集起来赋给<code>name</code>字段。  </p>
<ol>
<li><p><code>//div[@class=&quot;product_name&quot;]</code></p>
</li>
<li><p><code>//div[@class=&quot;product_title&quot;]</code><br>后面使用<code>add_css()</code>或<code>add_value()</code>方法填充<code>stock</code>字段和<code>last_updated</code>字段。<br>最后，当所有数据被收集起来后，调用<code>ItemLoader.load_item()</code>方法，返回填充好的Item。</p>
<h3 id="输入和输出处理器"><a href="#输入和输出处理器" class="headerlink" title="输入和输出处理器"></a>输入和输出处理器</h3><p>一个Item Loader在每个字段中包含一个输入处理器和一个输出处理器。输入处理器处理通过<code>add_xpath()</code>,<code>add_css()</code>,<code>add_value()</code>提取的数据。输入处理器的结果会被ItemLoader收集并保存。在收集到所有数据后调用<code>ItemLoader.load_item()</code>方法来填充并获得填充后的Item对象。输出处理器调用会使用之前收集到的数据，它处理的完后的结果会被分配到item中。<br>让我们看一个例子来说明如何输入和输出处理器被一个特定的字段调用：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">l = ItemLoader(Product(), some_selector)</span><br><span class="line">l.add_xpath(<span class="string">&#x27;name&#x27;</span>, xpath1) <span class="comment"># (1)</span></span><br><span class="line">l.add_xpath(<span class="string">&#x27;name&#x27;</span>, xpath2) <span class="comment"># (2)</span></span><br><span class="line">l.add_css(<span class="string">&#x27;name&#x27;</span>, css) <span class="comment"># (3)</span></span><br><span class="line">l.add_value(<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;test&#x27;</span>) <span class="comment"># (4)</span></span><br><span class="line"><span class="keyword">return</span> l.load_item() <span class="comment">#</span></span><br></pre></td></tr></table></figure>    
<p>看看发生了什么：  </p>
</li>
<li><p>数据从<code>xpath1</code>提取出来，通过<code>name</code>字段的输入处理器。输入处理器的结果会被Item Loader收集和保存起来（并没有分配给 item）</p>
</li>
<li><p>数据从<code>xpath2</code>提取出来，通过和（1）一样的输入处理器。输入处理器的结果会被附加到（1）的结果上（如果有）。</p>
</li>
<li><p>这步和上面类似，除了数据是通过<code>CSS selector</code>提取。通过和（1），（2）一样的输入处理器。输入处理器的结果会被附加到（1）,（2）上（如果有）。</p>
</li>
<li><p>这步和上面类似，除了数据值是直接被指定的，代替了从<code>XPath</code>表达式和<code>CSS</code>选择器提取。这值依然被传递给输入处理器。这个例子中，值不是可迭代对象，它会被转换为可迭代对象在传给输入器之前，因为输入器只接收可迭代的对象。</p>
</li>
<li><p>（1），（2），（3），（4）步收集的数据会传给<code>name</code>字段的输出处理器。输出处理器的结果会被分配给item的<code>name</code>字段。</p>
</li>
</ol>
<p>只的注意是处理器只是个可调用的对象，它调用需要被解析的数据，返回解析后的值。你可以使用任何函数作为输入和输出处理器。唯一的要求是它们必须接收一个而且只能接收一个可迭代的对象作为参数。<br>输入和输出处理器必须接收一个可迭代的对象作为它们第一个参数。而它们函数的输出可以是任何值。输入处理器的结果会被插入一个包含有这个字段收集的值的列表中。输出处理器的结果会作为最终的值分配给item。<br>Scrapy 附带了一些便捷的通用处理器。  </p>
<h3 id="声明Item-Loader"><a href="#声明Item-Loader" class="headerlink" title="声明Item Loader"></a>声明Item Loader</h3><p>Item Loader可以像Item一样被定义，使用class语法定义。例子：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> TakeFirst, MapCompose, Join</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ProductLoader</span>(<span class="params">ItemLoader</span>):</span></span><br><span class="line">	default_output_processor = TakeFirst()</span><br><span class="line">	name_in = MapCompose(unicode.title)</span><br><span class="line">	name_out = Join()</span><br><span class="line">	price_in = MapCompose(unicode.strip)</span><br><span class="line">	<span class="comment"># ...</span></span><br></pre></td></tr></table></figure>        
<p>就如你看到的，输入处理器使用<code>_in</code>后缀定义而输出处理器使用<code>_out</code>后缀语法定义。你也可以使用<code>ItemLoader.default_input_processor</code>和<code>ItemLoader.default_output_processor</code>属性定义。  </p>
<h3 id="声明输入和输出处理器"><a href="#声明输入和输出处理器" class="headerlink" title="声明输入和输出处理器"></a>声明输入和输出处理器</h3><p>如上小结所示，输入和输出处理器能够在定义Item Loader的class中声明，这是一种很通用的声明处理器的方法。<br>然而很多地方，你可以在定义Item Field 元数据时，指明使用的输入和输出处理器。例如：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> scrapy.loader.processors <span class="keyword">import</span> Join, MapCompose, TakeFirst</span><br><span class="line"><span class="keyword">from</span> w3lib.html <span class="keyword">import</span> remove_tags</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">filter_price</span>(<span class="params">value</span>):</span></span><br><span class="line">	<span class="keyword">if</span> value.isdigit():</span><br><span class="line">	<span class="keyword">return</span> value</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Product</span>(<span class="params">scrapy.Item</span>):</span></span><br><span class="line">	name = scrapy.Field(</span><br><span class="line">		input_processor=MapCompose(remove_tags),</span><br><span class="line">		output_processor=Join(),</span><br><span class="line">	)</span><br><span class="line">	price = scrapy.Field(</span><br><span class="line">		input_processor=MapCompose(remove_tags, filter_price),</span><br><span class="line">		output_processor=TakeFirst(),</span><br><span class="line">	)</span><br></pre></td></tr></table></figure>        
<p>调用：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.loader <span class="keyword">import</span> ItemLoader</span><br><span class="line">il = ItemLoader(item=Product())</span><br><span class="line">il.add_value(<span class="string">&#x27;name&#x27;</span>, [<span class="string">u&#x27;Welcome to my&#x27;</span>, <span class="string">u&#x27;&lt;strong&gt;website&lt;/strong&gt;&#x27;</span>])</span><br><span class="line">il.add_value(<span class="string">&#x27;price&#x27;</span>, [<span class="string">u&#x27;&amp;euro;&#x27;</span>, <span class="string">u&#x27;&lt;span&gt;1000&lt;/span&gt;&#x27;</span>])</span><br><span class="line">il.load_item()</span><br></pre></td></tr></table></figure>    
<p>输入和输出处理器的优先处理顺序是：</p>
<ol>
<li>Item Loader字段指明的属性：<code>field_in</code>和<code>field_out</code>优先级最高。</li>
<li>字段元数据（input_processor和output_processor键）</li>
<li>Item Loader默认：<code>ItemLoader.default_input_processor()</code>和<code>ItemLoader.default_output_processor()</code>最低。</li>
</ol>
<h3 id="ItemLoader-Objects"><a href="#ItemLoader-Objects" class="headerlink" title="ItemLoader Objects"></a>ItemLoader Objects</h3><blockquote>
<p>class scrapy.loader.ItemLoader([item,selector,response,]**kwargs)</p>
</blockquote>
<p>提供需要处理的一个Item,会返回一个新的Item Loader。如果没有提供Item,实例化时会自动使用<code>default_item_class</code>。<br>当使用__selector__和__response__实例话时，<code>ItemLoader</code>类提供了方便的机制从页面中使用选择器提取数据。<br>参数:  </p>
<ul>
<li>item — 需要处理的Item 实例</li>
<li>selector — 提取数据的选择器，可以使用<code>add_xpath()</code>,<code>add_css()</code>,<code>replace_xpath()</code>方法。</li>
<li>response — response会使用<code>default_selector_class</code>作为选择器如果在构造时没有传递<code>selector</code>参数。</li>
</ul>
<p>实例话后的<code>ItemLoader</code>拥有下面这些方法：  </p>
<blockquote>
<p><strong>get_value(value, *processors,**kwargs)</strong></p>
</blockquote>
<p>处理<code>value</code>通过给定处理器和关键字参数。<br>有效的关键字:  </p>
<ul>
<li>Parameters: re(字符串或正则表达式) 一个正则表达式用于<code>extarct_regex()</code>方法从value中提取数据，在交给处理器之前。    <code>from scrapy.loader.processors import TakeFirst</code><br>  <code>loader.get_value(u&#39;name: foo&#39;, TakeFirst(), unicode.upper, re=&#39;name: (.+)&#39;)</code>    <blockquote>
<p><strong>add_value(field_name,value,*processors, **kwargs)</strong></p>
</blockquote>
</li>
</ul>
<p>将<code>value</code>添加到给定字段上。<br>这个值首先将经过给<code>get_value()</code>如果提供了<code>processors</code>和<code>kwargs</code>，然后通过字段的输入处理器。输入处理器的结果会附加上这个字段已经收集到的数据上。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loader.add_value(<span class="string">&#x27;name&#x27;</span>, <span class="string">u&#x27;Color TV&#x27;</span>)</span><br><span class="line">loader.add_value(<span class="string">&#x27;colours&#x27;</span>, [<span class="string">u&#x27;white&#x27;</span>, <span class="string">u&#x27;blue&#x27;</span>])</span><br><span class="line">loader.add_value(<span class="string">&#x27;length&#x27;</span>, <span class="string">u&#x27;100&#x27;</span>)</span><br><span class="line">loader.add_value(<span class="string">&#x27;name&#x27;</span>, <span class="string">u&#x27;name: foo&#x27;</span>, TakeFirst(), re=<span class="string">&#x27;name: (.+)&#x27;</span>)</span><br><span class="line">loader.add_value(<span class="literal">None</span>, &#123;<span class="string">&#x27;name&#x27;</span>: <span class="string">u&#x27;foo&#x27;</span>, <span class="string">&#x27;sex&#x27;</span>: <span class="string">u&#x27;male&#x27;</span>&#125;)</span><br></pre></td></tr></table></figure>    
<blockquote>
<p>replace_value(field_name, value, *processors, **kwargs)</p>
</blockquote>
<p>类似于<code>add_value()</code>但是用value替换已经收集到的数据。</p>
<blockquote>
<p>get_xpath(xpath, *processors, **kwargs)</p>
</blockquote>
<p>类似<code>ItemLoader.get_value()</code>但是接收一个XPaht 替换value,该值用于从与此关联的选择器中提取unicode字符串列表ItemLoader。  </p>
<ul>
<li>Parameters: <ul>
<li>xpath(str) - 从XPath提取数据</li>
<li>re(字符串或正则表达式)- 这个正则表达式用于提取数据从选择的XPath 区域。</li>
</ul>
</li>
</ul>
<pre><code>```# HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;```  
```loader.get\_xpath(&#39;//p[@class=&quot;product-name&quot;]&#39;)```
```# HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;```
```loader.get\_xpath(&#39;//p[@id=&quot;price&quot;]&#39;, TakeFirst(), re=&#39;the price is (.*)&#39;)```
</code></pre>
<blockquote>
<p>add_xpath(field_name,xpath, *processors, **kwargs)</p>
</blockquote>
<p>类似于<code>ItemLoader.add_value()</code>但是使用XPath代替value,该值用于从与此关联的选择器中提取unicode字符串列表ItemLoader。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HTML snippet: &lt;p class=&quot;product-name&quot;&gt;Color TV&lt;/p&gt;</span></span><br><span class="line">loader.add_xpath(<span class="string">&#x27;name&#x27;</span>, <span class="string">&#x27;//p[@class=&quot;product-name&quot;]&#x27;</span>)</span><br><span class="line"><span class="comment"># HTML snippet: &lt;p id=&quot;price&quot;&gt;the price is $1200&lt;/p&gt;</span></span><br><span class="line">loader.add_xpath(<span class="string">&#x27;price&#x27;</span>, <span class="string">&#x27;//p[@id=&quot;price&quot;]&#x27;</span>, re=<span class="string">&#x27;the price is (.*)&#x27;</span>)</span><br></pre></td></tr></table></figure>    
<blockquote>
<p>replace_xpah(field_name, value, *processors, **kwargs)</p>
</blockquote>
<p>类似于<code>add_xpath()</code>。</p>
<h2 id="Item-Pipeline-1"><a href="#Item-Pipeline-1" class="headerlink" title="Item Pipeline"></a><a target="_blank" rel="noopener" href="https://doc.scrapy.org/en/latest/topics/item-pipeline.html">Item Pipeline</a></h2><p>当item被spider收集后，它会送到Item Pipeline经过几个连续的组件处理。<br>每个Item pipeline组件是个实现了一个简单方法Python 类。它接收一个item，对它做些操作，已决定item是继续传给pipeline或者销毁不再做处理。<br>典型的item pipelines有：  </p>
<ul>
<li>清理HTML数据</li>
<li>确认爬取的数据</li>
<li>检测是否重复</li>
<li>将item存储到数据库</li>
</ul>
<h3 id="写个item-pipeline"><a href="#写个item-pipeline" class="headerlink" title="写个item pipeline"></a>写个item pipeline</h3><p>每个item pipeline 组件是个Python 类，它必须实现下面这个方法：  </p>
<blockquote>
<p><strong>process_item(self,item,spider)</strong></p>
</blockquote>
<p>item pipeline组件会调用这个方法。这方法必须返回一个dict数据或者返回一个Item对象。或一个Twisted Deferred,或一个<code>DropItem</code>异常。丢弃的items不会在后面的pipeline处理。<br>参数：  </p>
<ul>
<li>item — 被爬取的item</li>
<li>spider — 爬取item的spider</li>
</ul>
<p>可以选择实现下面这些方法：  </p>
<blockquote>
<p><strong>open_spider(self,spider)</strong></p>
</blockquote>
<p>当spider被打开时这个方法会被调用。<br>参数：  </p>
<ul>
<li>spider — 被打开的spider</li>
</ul>
<blockquote>
<p><strong>close_spider(self, spider)</strong></p>
</blockquote>
<p>当spider被关闭时调用。<br>参数：  </p>
<ul>
<li>spider — 被关闭的spider</li>
</ul>
<blockquote>
<p><strong>from_crawler(cls, crawler)</strong></p>
</blockquote>
<p>如果它有，这应该是个类方法当<code>Crawler</code>实现一个pipeline时调用。<br>它必须返回一个实现了的pipleline。<code>Crawler</code>对象提供了 Scrapy 所有核心组件访问，例如设置和信号。这是pipeline访问并挂载功能到Scrapy唯一的方式。<br>参数：  </p>
<ul>
<li>crawler — crawler给pipeline使用</li>
</ul>
<h3 id="去掉没有价格的item"><a href="#去掉没有价格的item" class="headerlink" title="去掉没有价格的item"></a>去掉没有价格的item</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PricePipeline</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">vat_factor = <span class="number">1.15</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">	<span class="keyword">if</span> item[<span class="string">&#x27;price&#x27;</span>]:</span><br><span class="line">		<span class="keyword">if</span> item[<span class="string">&#x27;price_excludes_vat&#x27;</span>]:</span><br><span class="line">			item[<span class="string">&#x27;price&#x27;</span>] = item[<span class="string">&#x27;price&#x27;</span>] * self.vat_factor</span><br><span class="line">		<span class="keyword">return</span> item</span><br><span class="line">	<span class="keyword">else</span>:</span><br><span class="line">		<span class="keyword">raise</span> DropItem(<span class="string">&quot;Missing price in %s&quot;</span> % item)</span><br></pre></td></tr></table></figure>            
<p>上面的pipeline调整item<code>price</code>属性如果包含<code>price_excludes_vat</code>属性，然后抛弃没有<code>price</code>属性的items。</p>
<h3 id="将items写入JSON文件"><a href="#将items写入JSON文件" class="headerlink" title="将items写入JSON文件"></a>将items写入JSON文件</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">JsonWriterPipeline</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">		self.file = <span class="built_in">open</span>(<span class="string">&#x27;items.jl&#x27;</span>, <span class="string">&#x27;w&#x27;</span>)</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">		self.file.close()</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">		line = json.dumps(<span class="built_in">dict</span>(item)) + <span class="string">&quot;\n&quot;</span></span><br><span class="line">		self.file.write(line)</span><br><span class="line">		<span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>            
<p>上面的pipeline将所有spiders的items存入到一个<code>item.jl</code>文件中。每一行包含一条序列化后的item。<br>__注意：__JsonWriterPipeline目的只是为了介绍怎样写入item pipeline。如果你真的想将所有 items 存储到JSON file你需要使用<a target="_blank" rel="noopener" href="https://doc.scrapy.org/en/latest/topics/feed-exports.html#topics-feed-exports">Feed exports</a>。</p>
<h3 id="写入-MongoDB"><a href="#写入-MongoDB" class="headerlink" title="写入 MongoDB"></a>写入 MongoDB</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeline</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line">	collection_name = <span class="string">&#x27;scrapy_items&#x27;</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, mongo_uri, mongo_db</span>):</span></span><br><span class="line">		self.mongo_uri = mongo_uri</span><br><span class="line">		self.mongo_db = mongo_db</span><br><span class="line"><span class="meta">	@classmethod</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler</span>):</span></span><br><span class="line">		<span class="keyword">return</span> cls(</span><br><span class="line">			mongo_uri=crawler.settings.get(<span class="string">&#x27;MONGO_URI&#x27;</span>),</span><br><span class="line">			mongo_db=crawler.settings.get(<span class="string">&#x27;MONGO_DATABASE&#x27;</span>, <span class="string">&#x27;items&#x27;</span>)</span><br><span class="line">		)</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">open_spider</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">		self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">		self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">close_spider</span>(<span class="params">self, spider</span>):</span></span><br><span class="line">		self.client.close()</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">		self.db[self.collection_name].insert_one(<span class="built_in">dict</span>(item))</span><br><span class="line">		<span class="keyword">return</span> </span><br></pre></td></tr></table></figure>            
<p>上面的例子会将items写入MongoDB通过pymongo。MongoDB的地址和数据库名称是由Scrapy settings指定。MongoDB 集合名称就是 item class。<br>上面的例子只是为了指明怎么使用<code>from_crawler()</code>方法和怎样清除资源。</p>
<h3 id="拿到item的快照"><a href="#拿到item的快照" class="headerlink" title="拿到item的快照"></a>拿到item的快照</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">import</span> hashlib</span><br><span class="line"><span class="keyword">from</span> urllib.parse <span class="keyword">import</span> quote</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ScreenshotPipeline</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	<span class="string">&quot;&quot;&quot;Pipeline that uses Splash to render screenshot of</span></span><br><span class="line"><span class="string">	every Scrapy item.&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">	SPLASH_URL = <span class="string">&quot;http://localhost:8050/render.png?url=&#123;&#125;&quot;</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">		encoded_item_url = quote(item[<span class="string">&quot;url&quot;</span>])</span><br><span class="line">		screenshot_url = self.SPLASH_URL.<span class="built_in">format</span>(encoded_item_url)</span><br><span class="line">		request = scrapy.Request(screenshot_url)</span><br><span class="line">		dfd = spider.crawler.engine.download(request, spider)</span><br><span class="line">		dfd.addBoth(self.return_item, item)</span><br><span class="line">		<span class="keyword">return</span> dfd</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">return_item</span>(<span class="params">self, response, item</span>):</span></span><br><span class="line">		<span class="keyword">if</span> response.status != <span class="number">200</span>:</span><br><span class="line">			<span class="comment"># Error happened, return item.</span></span><br><span class="line">			<span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Save screenshot to file, filename will be hash of url.</span></span><br><span class="line">		url = item[<span class="string">&quot;url&quot;</span>]</span><br><span class="line">		url_hash = hashlib.md5(url.encode(<span class="string">&quot;utf8&quot;</span>)).hexdigest()</span><br><span class="line">		filename = <span class="string">&quot;&#123;&#125;.png&quot;</span>.<span class="built_in">format</span>(url_hash)</span><br><span class="line">		<span class="keyword">with</span> <span class="built_in">open</span>(filename, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">			f.write(response.body)</span><br><span class="line"></span><br><span class="line">		<span class="comment"># Store filename in item.</span></span><br><span class="line">		item[<span class="string">&quot;screenshot_filename&quot;</span>] = filename</span><br><span class="line">		<span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>            
<p>上面例子示范了<code>process_item()</code>方法如何返回一个延迟（<a target="_blank" rel="noopener" href="https://twistedmatrix.com/documents/current/core/howto/defer.html">Deferred</a>）方法。例子使用了<a target="_blank" rel="noopener" href="https://splash.readthedocs.io/en/stable/"><code>Splash</code></a>渲染item url的快照。Pipeline确保请求在本地运行<code>Splash</code>实例。当请求下载结束后，延迟方法会被触发。它保存了item到一个文件中，文件名就是item。</p>
<h3 id="去重"><a href="#去重" class="headerlink" title="去重"></a>去重</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DuplicatesPipeline</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">		self.ids_seen = <span class="built_in">set</span>()</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">process_item</span>(<span class="params">self, item, spider</span>):</span></span><br><span class="line">		<span class="keyword">if</span> item[<span class="string">&#x27;id&#x27;</span>] <span class="keyword">in</span> self.ids_seen:</span><br><span class="line">			<span class="keyword">raise</span> DropItem(<span class="string">&quot;Duplicate item found: %s&quot;</span> % item)</span><br><span class="line">		<span class="keyword">else</span>:</span><br><span class="line">			self.ids_seen.add(item[<span class="string">&#x27;id&#x27;</span>])</span><br><span class="line">		<span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>            
<p>上面的例子过滤重复的item,抛弃那些一ing处理过的items。</p>
<h3 id="激活Item-Pipeline-组件"><a href="#激活Item-Pipeline-组件" class="headerlink" title="激活Item Pipeline 组件"></a>激活Item Pipeline 组件</h3><p>激活Item Pipeline组件你必须添加 class到<a target="_blank" rel="noopener" href="https://doc.scrapy.org/en/latest/topics/settings.html#std:setting-ITEM_PIPELINES"><code>Item_PIPELINES</code></a>配置中。例如下：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">	<span class="string">&#x27;myproject.pipelines.PricePipeline&#x27;</span>: <span class="number">300</span>,</span><br><span class="line">	<span class="string">&#x27;myproject.pipelines.JsonWriterPipeline&#x27;</span>: <span class="number">800</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>    
<p>分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内</p>
<h2 id="Settings"><a href="#Settings" class="headerlink" title="Settings"></a><a target="_blank" rel="noopener" href="https://doc.scrapy.org/en/latest/topics/settings.html">Settings</a></h2><p>Scrapy settings 允许你自己定义所有Scrapy 组件的行为。包括核心库，扩展，pipeline和spider。<br>这些基础结构的settings为全局提供了key-value形式的命名空间，代码可以用拉取配置的值。这些settting能通过不同的机制填充。   </p>
<h3 id="指定设定"><a href="#指定设定" class="headerlink" title="指定设定"></a>指定设定</h3><p>当使用Scrapy时，你需要指定你所使用的setting。这可以通过使用环境变量:<code>SCRAPY_SETTRINGS_MODULE</code>来完成。<br><code>SCRAPY_SETTRINGS_MODULE</code>必须以Python路径语法编写，例如:<code>myproject.settings</code>。</p>
<h3 id="填写settings"><a href="#填写settings" class="headerlink" title="填写settings"></a>填写settings</h3><p>填写Settings有不同的机制，每个机制有其优先顺序。下面列表以降序的顺序给出了优先级：  </p>
<ul>
<li>命令行选项(Command line Options)(最高优先级)</li>
<li>每个spider的设定</li>
<li>项目设定模块(Project settings module)</li>
<li>命令默认设定模块(Default settings per-command)</li>
<li>全局默认设定(Default global settings) (最低优先级)</li>
</ul>
<h3 id="命令行选项-Command-line-options"><a href="#命令行选项-Command-line-options" class="headerlink" title="命令行选项(Command line options)"></a>命令行选项(Command line options)</h3><p>命令行传入的参数具有最高的优先级。 您可以使用command line 选项 -s (或 –set) 来覆盖一个(或更多)选项。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy crawl myspider -s LOG_FILE=scrapy.log</span><br></pre></td></tr></table></figure>    
<h3 id="spider内指定Settings"><a href="#spider内指定Settings" class="headerlink" title="spider内指定Settings"></a>spider内指定Settings</h3><p>spider可以设定它们自己settings，它会覆盖项目的settings。它们可以通过<code>scrapy.spiders.Spider.custom_settings</code>属性设定。  </p>
<h3 id="项目settings"><a href="#项目settings" class="headerlink" title="项目settings"></a>项目settings</h3><p>项目设定模块是你Scrapy项目标准配置文件。这个是设定你大多数settings的地方。标准的Scrapy项目，这个方式你需要创建一个<code>settings.py</code>文件在你目录中。</p>
<h3 id="命令默认settings"><a href="#命令默认settings" class="headerlink" title="命令默认settings"></a>命令默认settings</h3><p>每个Scrapy tool拥有自己的默认设置，并覆盖全局的默认设置。这个自定义的通用设置指定在通用class的<code>default_settings</code>属性上。</p>
<h3 id="默认全局settings"><a href="#默认全局settings" class="headerlink" title="默认全局settings"></a>默认全局settings</h3><p>全局默认设定存储在 <code>scrapy.settings.default_settings</code> 模块， 并在 内置设定参考手册 部分有所记录。</p>
<h3 id="访问settings"><a href="#访问settings" class="headerlink" title="访问settings"></a>访问settings</h3><p>在spider中，settings可用通过<code>self.settings</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MySpider</span>(<span class="params">scrapy.Spider</span>):</span></span><br><span class="line">	name = <span class="string">&#x27;myspider&#x27;</span></span><br><span class="line">	start_urls = [<span class="string">&#x27;http://example.com&#x27;</span>]</span><br><span class="line"></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">parse</span>(<span class="params">self, response</span>):</span></span><br><span class="line">		<span class="built_in">print</span>(<span class="string">&quot;Existing settings: %s&quot;</span> % self.settings.attributes.keys())</span><br></pre></td></tr></table></figure>            
<p>__注意：__这个settings属性是在spider初始化后，Spider类中设置的。如果你想使用settings在初始化之前（例如：你的spider<code>__init()__</code>方法），你必须重写<code>from\_crawler()</code>方法。<br>settings可以在Crawler的<code>scrapy.crawler.Crawler.settings</code>属性中接收到。它可以在扩展，中间件，item pipeline的<code>from_crawler</code>方法中取到。  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyExtension</span>(<span class="params"><span class="built_in">object</span></span>):</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, log_is_enabled=<span class="literal">False</span></span>):</span></span><br><span class="line">		<span class="keyword">if</span> log_is_enabled:</span><br><span class="line">			<span class="built_in">print</span>(<span class="string">&quot;log is enabled!&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">	@classmethod</span></span><br><span class="line">	<span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span>(<span class="params">cls, crawler</span>):</span></span><br><span class="line">		settings = crawler.settings</span><br><span class="line">		<span class="keyword">return</span> cls(settings.getbool(<span class="string">&#x27;LOG_ENABLED&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h2 id="命令行工具"><a href="#命令行工具" class="headerlink" title="命令行工具"></a><a target="_blank" rel="noopener" href="https://doc.scrapy.org/en/latest/topics/commands.html">命令行工具</a></h2><p>Scrapy是通过<code>scrapy</code>命令行控制的。<br>Scrapy tool 针对不同的目的提供了多个命令，每个命令支持不同的参数和选项。  </p>
<h3 id="调整设置"><a href="#调整设置" class="headerlink" title="调整设置"></a>调整设置</h3><p>Scrapy将会在以下路径中寻找记录了配置参数的 <code>scrapy.cfg</code> 文件, 该文件以ini的方式记录:   </p>
<ol>
<li><code>/etc/scrapy.cfg</code> 或 <code>c:\scrapy\scrapy.cfg</code>(系统层面),</li>
<li><code>~/.config/scrapy.cfg</code> ($XDG_CONFIG_HOME) 及 <code>~/.scrapy.cfg ($HOME)</code> 作为全局(用户层面)设置, 以及</li>
<li>在scrapy项目根路径下的 scrapy.cfg (参考之后的章节).<br>从这些文件中读取到的设置按照以下的顺序合并: 用户定义的值具有比系统级别的默认值更高的优先级, 而项目定义的设置则会覆盖其他.  <h3 id="默认Scrapy项目结构"><a href="#默认Scrapy项目结构" class="headerlink" title="默认Scrapy项目结构"></a>默认Scrapy项目结构</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">scrapy.cfg</span><br><span class="line">myproject/</span><br><span class="line">	__init__.py</span><br><span class="line">	items.py</span><br><span class="line">	pipelines.py</span><br><span class="line">	settings.py</span><br><span class="line">	spiders/</span><br><span class="line">		__init__.py</span><br><span class="line">		spider1.py</span><br><span class="line">		spider2.py</span><br><span class="line">		...</span><br></pre></td></tr></table></figure>            
<code>scrapy.cfg</code> 存放的目录被认为是 项目的根目录 。该文件中包含python模块名的字段定义了项目的设置。  <figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[settings]</span><br><span class="line">default = myproject.settings</span><br></pre></td></tr></table></figure>    
<h3 id="创建项目"><a href="#创建项目" class="headerlink" title="创建项目"></a>创建项目</h3>一般来说<code>scrapy</code>工具第一件事是创建Scrapy项目：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy startproject myproject</span><br></pre></td></tr></table></figure>    
<h3 id="创建spider"><a href="#创建spider" class="headerlink" title="创建spider"></a>创建spider</h3>创建spider快捷方法<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># scrapy genspider [-t template] &lt;name&gt; &lt;domain&gt;</span><br><span class="line">scrapy genspider myspider mydomain.com</span><br></pre></td></tr></table></figure>    
<h3 id="调用-spider"><a href="#调用-spider" class="headerlink" title="调用 spider"></a>调用 spider</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#scrapy crawl &lt;spider&gt;</span><br><span class="line">scrapy crawl myspider</span><br></pre></td></tr></table></figure>    
<h3 id="列出-spider"><a href="#列出-spider" class="headerlink" title="列出 spider"></a>列出 spider</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scrapy list</span><br><span class="line"># myspider</span><br></pre></td></tr></table></figure>    
<h3 id="通过链接加载页面"><a href="#通过链接加载页面" class="headerlink" title="通过链接加载页面"></a>通过链接加载页面</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#scrapy fetch &lt;url&gt;</span><br><span class="line">scrapy fetch --nolog http://www.example.com/some/page.html</span><br></pre></td></tr></table></figure>    
使用Scrapy下载器(downloader)下载给定的URL，并将获取到的内容送到标准输出。<h3 id="spider方式展示页面"><a href="#spider方式展示页面" class="headerlink" title="spider方式展示页面"></a>spider方式展示页面</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#scrapy view &lt;url&gt;</span><br><span class="line">scrapy view http://www.example.com/some/page.html</span><br></pre></td></tr></table></figure>    
在浏览器中打开给定的URL，并以Scrapy spider获取到的形式展现。 有些时候spider获取到的页面和普通用户看到的并不相同。 因此该命令可以用来检查spider所获取到的页面，并确认这是您所期望的。<h3 id="解析链接"><a href="#解析链接" class="headerlink" title="解析链接"></a>解析链接</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scrapy parse &lt;url&gt; [options]</span><br></pre></td></tr></table></figure>    
获取给定的URL并使用相应的spider分析处理。如果您提供 –callback 选项，则使用spider的该方法处理，否则使用 parse 。  </li>
</ol>
<ul>
<li>–spider=SPIDER: 跳过自动检测spider并强制使用特定的spider  </li>
<li>–a NAME=VALUE: 设置spider的参数(可能被重复)</li>
<li>–callback or -c: spider中用于解析返回(response)的回调函数</li>
<li>–pipelines: 在pipeline中处理item</li>
<li>–rules or -r: 使用 CrawlSpider 规则来发现用来解析返回(response)的回调函数</li>
<li>–noitems: 不显示爬取到的item</li>
<li>–nolinks: 不显示提取到的链接</li>
<li>–nocolour: 避免使用pygments对输出着色</li>
<li>–depth or -d: 指定跟进链接请求的层次数(默认: 1)</li>
<li>–verbose or -v: 显示每个请求的详细信息</li>
</ul>

      
    </div>

    
      
      

  <div class="post-copyright">
    <p class="copyright-item">
      <span>作者: </span>
      <span>Fynn</span>
    </p>
    <p class="copyright-item">
      <span>来源: </span>
      <a href="https://fynn90.github.io">https://fynn90.github.io</a>
    </p>
    <p class="copyright-item">
      <span>链接: </span>
      <a href="https://fynn90.github.io/2017/11/19/scrapy%E5%85%A5%E9%97%A8/">https://fynn90.github.io/2017/11/19/scrapy%E5%85%A5%E9%97%A8/</a>
    </p>

    <p class="copyright-item lincese">
      
      本文采用<a rel="license" href="http://creativecommons.org/licenses/by-nc/4.0/" target="_blank">知识共享署名-非商业性使用 4.0 国际许可协议</a>进行许可
    </p>
  </div>



      
      
  <div class="post-reward">
    <input type="checkbox" name="reward" id="reward" hidden />
    <label class="reward-button" for="reward">赞赏支持</label>
    <div class="qr-code">
      
      
        <label class="qr-code-image" for="reward">
          <img class="image" src="/image/reward/wechat.png" title="wechat">
          <div>wechat</div>
        </label>
      
      
        <label class="qr-code-image" for="reward">
          <img class="image" src="/image/reward/alipay.png" title="alipay">
          <div>alipay</div>
        </label>
      
    </div>
  </div>

    

    
      <footer class="post-footer">
        
          <div class="post-tags">
            
              <a href="/tags/Scrapy/">Scrapy</a>
            
          </div>
        
        
        
  <nav class="post-nav">
    
      <a class="prev" href="/2017/11/23/markdown%E6%89%8B%E5%86%8C/">
        <i class="iconfont icon-left"></i>
        <span class="prev-text nav-default">Markdown手册</span>
        <span class="prev-text nav-mobile">上一篇</span>
      </a>
    
    
      <a class="next" href="/2017/11/11/python%E7%88%AC%E8%99%AB%E5%85%A5%E9%97%A8/">
        <span class="next-text nav-default">Python爬虫入门</span>
        <span class="prev-text nav-mobile">下一篇</span>
        <i class="iconfont icon-right"></i>
      </a>
    
  </nav>

      </footer>
    

  </article>


          </div>
          
  <div class="comments" id="comments">
    
  </div>


        </div>  
      </main>

      <footer id="footer" class="footer">

  <div class="social-links">
    
      
        
          <a href="mailto:fynn.90@outlook.com" class="iconfont icon-email" title="email"></a>
        
      
    
      
    
      
    
      
    
      
        
          <a target="_blank" rel="noopener" href="https://www.linkedin.com/in/%E5%B8%86-%E9%82%93-17163589/" class="iconfont icon-linkedin" title="linkedin"></a>
        
      
    
      
        
          <a target="_blank" rel="noopener" href="https://plus.google.com/u/0/117459332873536225443" class="iconfont icon-google" title="google"></a>
        
      
    
      
        
          <a target="_blank" rel="noopener" href="https://github.com/fatefan" class="iconfont icon-github" title="github"></a>
        
      
    
      
        
          <a target="_blank" rel="noopener" href="http://www.weibo.com/306019091" class="iconfont icon-weibo" title="weibo"></a>
        
      
    
      
        
          <a target="_blank" rel="noopener" href="https://www.zhihu.com/people/FynnDeng/activities" class="iconfont icon-zhihu" title="zhihu"></a>
        
      
    
      
    
    
    
  </div>


<div class="copyright">
  
  <span class="copyright-year">
    
    &copy; 
     
      2017 - 
    
    2021

    <span class="heart">
      <i class="iconfont icon-heart"></i>
    </span>
    <span class="author">Fynn</span>
  </span>
  
</div>
      </footer>

      <div class="back-to-top" id="back-to-top">
        <i class="iconfont icon-up"></i>
      </div>
    </div>

    
  

  
  




    
  





  
    <script type="text/javascript" src="/lib/jquery/jquery-3.1.1.min.js"></script>
  

  
    <script type="text/javascript" src="/lib/slideout/slideout.js"></script>
  

  
    <script type="text/javascript" src="/lib/fancybox/jquery.fancybox.pack.js"></script>
  


    <script type="text/javascript" src="/js/src/even.js?v=1.1.2"></script>
<script type="text/javascript" src="/js/src/bootstrap.js?v=1.1.2"></script>

    
  </body>
</html>
